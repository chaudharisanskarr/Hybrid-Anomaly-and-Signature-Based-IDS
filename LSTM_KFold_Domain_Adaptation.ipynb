{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b301db6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\harsh\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: 'saint'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\harsh\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_encode.py:225\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\harsh\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_encode.py:165\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[1;34m(values, uniques)\u001b[0m\n\u001b[0;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[1;32mc:\\Users\\harsh\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_encode.py:165\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[1;32mc:\\Users\\harsh\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_encode.py:159\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[1;32m--> 159\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'saint'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[0;32m     30\u001b[0m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattack\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattack\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 31\u001b[0m test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattack\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mtransform(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattack\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Select features and labels\u001b[39;00m\n\u001b[0;32m     34\u001b[0m X \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattack\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\harsh\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\harsh\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:137\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _encode(y, uniques\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)\n",
      "File \u001b[1;32mc:\\Users\\harsh\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_encode.py:227\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 227\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[1;31mValueError\u001b[0m: y contains previously unseen labels: 'saint'"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load Data\n",
    "train_data = pd.read_csv(\"./KDDTrain+.txt\")\n",
    "test_data = pd.read_csv(\"./KDDTest+.txt\")\n",
    "\n",
    "# Define columns manually (assuming predefined feature names)\n",
    "columns = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment',\n",
    "           'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted',\n",
    "           'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
    "           'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n",
    "           'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
    "           'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "           'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "           'dst_host_srv_rerror_rate', 'attack', 'level']\n",
    "\n",
    "train_data.columns = columns\n",
    "test_data.columns = columns\n",
    "\n",
    "# Encode categorical variables\n",
    "# Encode categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "train_data['attack'] = label_encoder.fit_transform(train_data['attack'])\n",
    "\n",
    "# Ensure test labels are mapped properly\n",
    "test_data['attack'] = test_data['attack'].apply(lambda x: x if x in label_encoder.classes_ else 'unknown')\n",
    "label_encoder.classes_ = np.append(label_encoder.classes_, 'unknown')  # Add unknown class\n",
    "test_data['attack'] = label_encoder.transform(test_data['attack'])\n",
    "\n",
    "\n",
    "# Select features and labels\n",
    "X = train_data.drop(columns=['attack'])\n",
    "y = train_data['attack']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "y_categorical = to_categorical(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1839c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define LSTM model\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        LSTM(64, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661183ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_no = 1\n",
    "accuracy_per_fold = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_scaled):\n",
    "    print(f'Training Fold {fold_no}...')\n",
    "    X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "    y_train, y_val = y_categorical[train_index], y_categorical[val_index]\n",
    "    \n",
    "    model = create_model((X_train.shape[1], 1), y_categorical.shape[1])\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=1)\n",
    "    \n",
    "    accuracy_per_fold.append(history.history['val_accuracy'][-1])\n",
    "    fold_no += 1\n",
    "\n",
    "# Plot Accuracy Graph\n",
    "plt.plot(range(1, 6), accuracy_per_fold, marker='o', linestyle='-')\n",
    "plt.xlabel('Fold Number')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('K-Fold Cross Validation Accuracy')\n",
    "plt.show()\n",
    "\n",
    "print(\"Average Validation Accuracy: \", np.mean(accuracy_per_fold))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ead2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transfer Learning - Load Pretrained Model\n",
    "pretrained_model = create_model((X_train.shape[1], 1), y_categorical.shape[1])\n",
    "pretrained_model.load_weights('pretrained_lstm_weights.h5')\n",
    "\n",
    "# Fine-tune on new dataset\n",
    "pretrained_model.fit(X_scaled, y_categorical, epochs=5, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe82f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adversarially Robust Training (FGSM Attack)\n",
    "def fgsm_attack(model, images, labels, epsilon=0.1):\n",
    "    images = tf.convert_to_tensor(images)\n",
    "    labels = tf.convert_to_tensor(labels)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(images)\n",
    "        predictions = model(images)\n",
    "        loss = tf.keras.losses.categorical_crossentropy(labels, predictions)\n",
    "    gradient = tape.gradient(loss, images)\n",
    "    perturbed_images = images + epsilon * tf.sign(gradient)\n",
    "    return np.clip(perturbed_images, 0, 1)\n",
    "\n",
    "# Generate Adversarial Examples and Retrain\n",
    "X_adversarial = fgsm_attack(pretrained_model, X_scaled, y_categorical)\n",
    "pretrained_model.fit(X_adversarial, y_categorical, epochs=5, batch_size=32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
